<!DOCTYPE html>
<html>
<head>
    <title>Free-Form Flows</title>
    <link rel="stylesheet" href="../../../../main.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: "\\(", right: "\\)", display: false},
                    {left: "\\[", right: "\\]", display: true}
                ],
                throwOnError: false
            });
        });
    </script>
    <style>
        @media (max-width: 600px) {
            article {
                font-size: 16px;
            }

            .katex-display {
                overflow-x: auto;
                overflow-y: hidden;
            }
        }
    </style>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
    <div class="container">
        <nav>
            <a href="../../../../index.html">About Me</a>
            <a href="../../../../blog.html">Blog</a>
        </nav>

        <article>
            <h1>Free-Form Flows</h1>
            <time datetime="2024-12-30">December 30, 2024</time>

            Free-form flows are a new class of generative models which I developed with my collaborators at Heidelberg University. They are fast, flexible, and can be used for a wide range of applications. So far we have successfully generated images, molecules, tabular data, and a wide variety of data on manifolds, such as spheres, tori, and meshes.
            <br><br>
            Free-form flows are inspired by normalizing flows, but do away with the inflexible architecture. There are currently three papers that have been published on the topic:
            <ul>
                <li>
                    <a href="https://proceedings.mlr.press/v238/draxler24a.html">Free-Form Flows: Make Any Architecture a Normalizing Flow</a>
                </li>
                <li>
                    <a href="https://openreview.net/forum?id=kBNIx4Biq4">Lifting Architectural Constraints of Injective Flows</a>
                </li>
                <li>
                    <a href="https://arxiv.org/abs/2312.09852">Learning Distributions on Manifolds with Free-Form Flows</a>
                </li>
            </ul>
            The code to reproduce the experiments in the papers is available on <a href="https://github.com/vislearn/FFF">GitHub</a>.
            <br><br>
            Here I'll provide a high-level overview of the approach. I'm going to assume knowledge of the basics of machine learning and generative modeling, and I will state some things as fact which might not be obvious to everyone. I might cover some of the topics in more detail in future posts.
            
            <h2>Normalizing flows</h2>
            Most of the models that are used for generative modeling are based on maximum likelihood, whether in the exact or approximate sense. For example, autoregressive models like ChatGPT are trained to maximize the log-likelihood of the training data (at least in the pre-training phase) and text-to-image diffusion models like DALL-E can be interpreted as approximations to the exact maximum likelihood objective. Out of all the most popular generative models, only GANs are not based on maximum likelihood, and they have fallen out of favor in the last few years.
            <br><br>
            Normalizing flows take a special place in the landscape of exact maximum likelihood models, since they allow for very fast sampling and inference (by inference, I mean the computation of the log-likelihood). This is unusual, since most models are able to offer either fast sampling or fast inference, but not both.
            <br><br>
            In order to achieve this, normalizing flows are designed as invertible transport models, meaning that they transform the data distribution into a simple target distribution, such as a standard normal distribution. It's easy to see why sampling is fast: all we need to do is sample from the target distribution and apply the inverse of the flow to get a sample from the data distribution.
            <br><br>
            Inference is also fast because the log-likelihood can be computed analytically, thanks to the change of variables formula for probability densities. This formula states that, given an invertible transformation \( f \) that maps \( x \) to \( z \), the log-likelihood of \( x \) can be computed as
            \[
            \log p(x) = \log p(z) + \log \left| f'(x) \right|
            \]
            where \( \left| f'(x) \right| \) is the absolute value of the Jacobian determinant of the transformation, also know as the Jacobian.
            <br><br>
            A popular way to build normalizing flows is to stack multiple transformations, each parameterized by a neural network. Each transformation is itself invertible, with an easy-to-compute inverse and a simple Jacobian. This is often achieved by coupling blocks, which change half of the input at a time, dependent on the other half. Due to these requirements, the architecture of the normalizing flow is very constrained, and it is not always easy to incorporate domain knowledge or the latest advancements in neural network architecture design. For example, if you want to generate molecules, you might want to use a graph neural network which is equivariant to the symmetries of the molecule, but it takes a lot of effort to incorporate this into the architecture of a standard coupling-based normalizing flow.

            <h2>Free-form flows</h2>
            Enter free-form flows. These models allow you train a free-form neural network like a normalizing flow, that is, you end up with an invertible neural network trained to maximize the log-likelihood of the data. And we can use any architecture we wantâ€”there are no constraints, except for preserving the dimensionality of the input.
            <br><br>
            We achieve this by relaxing some requirements. Firstly, instead of using exactly invertible architectures, we settle for approximate invertibility, achieved by using two networks \( f \) and \( g \) which are coupled by a reconstruction loss. Secondly, in order to train via maximum likelihood, we need to compute the log-likelihood, which requires the Jacobian determinant. We approximate the gradient of this quantity via Jacobi's formula, which leaves us with a tractable objective.
            <br><br>
            The first point is easy to understand: this is simply the principle of autoencoders. The main difference is that autoencoders are usually thought of as a dimensionality reduction technique, while here we don't reduce the dimensionality, but use it to build a generative model.
            <br><br>
            The second point is more difficult to understand, as it requires chaining together several tricks. First we notice that the difficult part of the change of variables formula is the computation of the Jacobian. A key idea is to use Jacobi's formula, which states that for a matrix \( A \) parameterized by \( t \),
            \[
            \frac{d}{dt} \log \left| A(t) \right| = \text{tr} \left( A(t)^{-1} \frac{d A(t)}{dt} \right)
            \]
            When \( t \) is a parameter of a neural network, this formula allows us to compute the gradient of the Jacobian without having to compute the Jacobian itself. And since we are doing gradient descent, all we need is a way to compute the gradient of the loss function.
            <br><br>
            If we apply Jacobi's formula to the Jacobian of the flow, we get
            \[
            \nabla_\theta \log \left| f'(x) \right| = \text{tr} \left( f'(x)^{-1} \nabla_\theta f'(x) \right)
            \]
            where \( \theta \) are the parameters of the neural network. 
            <br><br>
            Inside the trace, we have the inverse of the Jacobian matrix \( f'(x) \), which is difficult to compute since it involves a matrix inversion. However, if \( f \) is invertible, we can use the fact that \( f'(x)^{-1} = \left( f^{-1} \right)' (f(x)) \), and that \( g \approx f^{-1} \) to approximate the gradient as
            \[
            \nabla_\theta \log \left| f'(x) \right| \approx \text{tr} \left( g'(f(x)) \nabla_\theta f'(x) \right)
            \]
            The remaining tricks are the use of stop-gradient, and the Hutchinson-Skilling trace estimator, which states that
            \[
            \text{tr} \left( A \right) \approx v^T A v
            \]
            for a random vector \( v \), as long as \( \mathbb{E} \left[ v v^T \right] = I \). A standard choice for \( v \) is a standard normal vector. The approximation is exact in expectation over \( v \).
            <br><br>
            This allows us to approximate the gradient of the Jacobian as
            \[
            \nabla_\theta \log \left| f'(x) \right| \approx v^T g'(f(x)) \nabla_\theta f'(x) v = \nabla_\theta \mathtt{SG}[v^T g'(f(x))] f'(x) v
            \]
            where \( \mathtt{SG} \) is the stop-gradient operator which prevents the gradient from flowing through the input. Stop-gradient is implemented in PyTorch with the \( \mathtt{detach} \) method.
            <br><br>
            In order to actually compute this quantity, we need to be able to compute the vector-Jacobian product \( v^T g'(f(x)) \) and the Jacobian-vector product \( f'(x) v \). Fortunately, this is easy to do with modern automatic differentiation libraries like PyTorch or JAX, since such products are implemented as fundamental operations.
            <br><br>
            In the end this gives a tractable surrogate objective for the log-likelihood, which has the same gradients (in expectation) as the exact objective, as long as the network \( g \) is close to being an exact inverse of \( f \).
            <br><br>
            Putting this all together, we get a loss function that can be minimized to train a free-form flow, which is given by
            \[
            \mathcal{L} = \mathbb{E}_{x \sim \mathcal{D}} \left[ -\log p(f(x)) - \mathtt{SG}[v^T g'(f(x))] f'(x) v + \beta \left\| g(f(x)) - x \right\|^2 \right]
            \]
            where \( \mathcal{D} \) is the dataset and \( \beta \) is a hyperparameter that controls the strength of the reconstruction loss.

            <h2>Recap</h2>
            Since that was a lot of math, let's recap the main points:
            <ul>
                <li>Normalizing flows are attractive because they are fast and trained with exact maximum likelihood, but they have to use specific architectures which don't always work well with domain knowledge and the latest advancements in neural network design.</li>
                <li>Free-form flows are an approximation to normalizing flows which don't place any constraints on the architecture. Approximate invertibility is achieved by using two networks \( f \) and \( g \) which are coupled by a reconstruction loss, and maximum likelihood training is approximated by using a set of tricks to estimate the gradient of the log-likelihood.</li>
                <li>These tricks are Jacobi's formula, substition of the inverse of \( f'(x) \) with \( g'(f(x)) \), stop-gradient, Hutchinson-Skilling trace estimation, and efficient computation of vector-Jacobian products and Jacobian-vector products.</li>
            </ul>

            <h2>Some applications</h2>
            <figure>
                <img src="qm9-training-samples.svg" alt="QM9 Training Samples" width="100%" height="400px" />
            </figure>
            In the figure above [Draxler et al., 2024], the training procedure is illustrated, with the maximum-likelihood and reconstruction losses. At the bottom are some generated samples from the flow, trained on molecules from the QM9 dataset. In this example, the model is a graph neural network which is equivariant to rotational and translational symmetries. Our model is about 20 times faster than a comparable normalizing flow for better performance, and more than 2000 times faster than a comparable diffusion model, though the quality of the generated samples is not as good. Nevertheless, because of the massive speed advantage, the time to generate a stable molecule is about 200 times faster than with a diffusion model.
            <br><br>
            We are also able to adapt free-form flows to become bottleneck models, meaning that the latent dimension is lower than the input dimension [Sorrenson et al., 2024a]. This is particularly useful for generative modeling of distributions where the intrinsic dimension is lower than the input dimension, such as in image data. We find that the model performs similarly to a comparable variational autoencoder on image generation, at the same speed. We encountered some difficulties in the training and theoretical interpretation of the model, but hope to improve the quality of the generated images in future work.

            <figure>
                <img src="manifold-distributions.jpg" alt="Manifold Distributions" width="100%" />
            </figure>
            It turns out to be relatively straightforward to adapt free-form flows to Riemannian manifolds. In this figure [Sorrenson et al., 2024b], we see some examples of distributions on manifolds, generated by a free-form flow. The manifolds shown here are a sphere, a torus, the hyperbolic plane, and a mesh shaped like a bunny (there are others in the paper, including higher-dimensional manifolds). We find that our models are much more performant than comparable normalizing flows specially designed for these manifolds, and much faster (generally 100 to 1000 times) than diffusion or flow matching models. 
            <br><br>
            Interestingly, we find for certain datasets that our model may lie on a different Pareto frontier than flow matching models with regard to the trade-off between log-likelihood and time to generate a sample (see figure below), which is a promising result as we aim to scale things up.
            <figure>
                <img src="general-nll-vs-time-with-reference.svg" alt="General NLL vs Time with Reference" width="100%" height="400px" />
            </figure>

            <h2>References</h2>
            <ul>
                <li>
                    Draxler et al., 2024: <a href="https://proceedings.mlr.press/v238/draxler24a.html">Free-Form Flows: Make Any Architecture a Normalizing Flow</a>
                </li>
                <li>
                    Sorrenson et al., 2024a: <a href="https://openreview.net/forum?id=kBNIx4Biq4">Lifting Architectural Constraints of Injective Flows</a>
                </li>
                <li>
                    Sorrenson et al., 2024b: <a href="https://arxiv.org/abs/2312.09852">Learning Distributions on Manifolds with Free-Form Flows</a>
                </li>
            </ul>

        </article>
    </div>
</body>
</html> 